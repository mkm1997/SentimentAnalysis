from keras.datasets import imdb
((XT,YT),(Xt,Yt))=imdb.load_data(num_words=10000)

idx_word=dict([value,key] for [key,value] in word_idx.items())
actual_review=' '.join([idx_word.get(idx-3,'?') for idx in XT[0]])
print(idx_word)

from keras.preprocessing import sequence

X_train=sequence.pad_sequences(XT,maxlen=500)
X_test=sequence.pad_sequences(Xt,maxlen=500)

from keras.layers import Embedding,SimpleRNN,Dense
from keras.models import Sequential

model=Sequential()
model.add(Embedding(10000,64))
model.add(SimpleRNN(32))
model.add(Dense(1,activation='sigmoid'))
model.summary()

model.compile(optimizer='rmsprop',loss='binary_crossentropy',metrics=['acc'])

from keras.callbacks import ModelCheckpoint #Reduces overfitting
from keras.callbacks import EarlyStopping #Save time
checkpoint=ModelCheckpoint("best_model.h5",monitor="val_loss",verbose=0, save_best_only=True,save_weights_only=False,mode='auto',period=1)

early_stop=EarlyStopping(monitor='val_acc',patience=1)
hist=model.fit(X_train,YT,epochs=10,batch_size=128,validation_split=0.2,callbacks=[checkpoint,early_stop])

from matplotlib import pyplot as plt
acc=hist.history['acc']
val_acc=hist.history['val_acc']

loss=hist.history['loss']
val_loss=hist.history['val_loss']

epochs=range(1,len(loss)+1)

plt.title("Loss vs epochs")
plt.plot(epochs,loss,label="Training loss")
plt.plot(epochs,val_loss,label="Validation loss")
plt.legend()
plt.show()


plt.title("Accuracy vs epochs")
plt.plot(epochs,acc,label="Training Acc")
plt.plot(epochs,val_acc,label="Validation Acc")
plt.legend()
plt.show()



model.evaluate(X_test,Yt)
model.evaluate(X_train,YT)
model.predict(X_test)


idx_word=dict([value,key] for [key,value] in word_idx.items())
actual_review=' '.join([idx_word.get(idx-3,'?') for idx in Xt[1]])
print(idx_word)

print(actual_review)
print(len(actual_review.split()))

model.load_weights("best_model.h5")

model.evaluate(X_test,Yt)

model.evaluate(X_train,YT)



